<!doctype html>
<html lang="en">
  <head>
    <title>EgoAllo</title>
    <meta
      content="We use egocentric SLAM poses and images to estimate 3D human body pose, height, and hands."
      name="description"
    />

    <!-- Usual metadata. -->
    <meta charset="UTF-8" />
    <link href="./favicon.png" rel="shortcut icon" type="image/x-icon" />
    <meta
      name="viewport"
      content="width=device-width, initial-scale=1, minimum-scale=1"
    />

    <!-- Open Graph / Facebook. -->
    <meta property="og:title" content="EgoAllo" />
    <meta
      property="og:description"
      content="We use egocentric SLAM poses and images to estimate 3D human body pose, height, and hands."
    />
    <meta property="og:type" content="website" />

    <!-- Twitter. -->
    <meta property="twitter:title" content="EgoAllo" />
    <meta
      property="twitter:description"
      content="We use egocentric SLAM poses and images to estimate 3D human body pose, height, and hands."
    />

    <!-- Webfont. -->
    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
    <link
      href="https://fonts.googleapis.com/css2?family=Inter:ital,opsz,wght@0,14..32,100..900;1,14..32,100..900&display=swap"
      rel="stylesheet"
    />

    <!-- Styles. -->
    <link
      href="https://cdn.jsdelivr.net/npm/modern-normalize@3.0.1/modern-normalize.min.css"
      rel="stylesheet"
    />
    <link
      rel="stylesheet"
      href="https://cdn.jsdelivr.net/npm/@tabler/icons@latest/iconfont/tabler-icons.min.css"
    />
    <link href="style.css" rel="stylesheet" type="text/css" />

    <!-- KaTeX -->
    <link
      rel="stylesheet"
      href="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.css"
      integrity="sha384-nB0miv6/jRmo5UMMR1wu3Gz6NLsoTkbqJghGIsx//Rlm+ZU03BU6SQNC66uf4l5+"
      crossorigin="anonymous"
    />
    <script
      defer
      src="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.js"
      integrity="sha384-7zkQWkzuo3B5mTepMUcHkMB5jZaolc2xDwL6VFqjFALcbeS9Ggm/Yr2r3Dy4lfFg"
      crossorigin="anonymous"
    ></script>
    <script
      defer
      src="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/contrib/auto-render.min.js"
      integrity="sha384-43gviWU0YVjaDtb/GhzOouOXtZMP/7XUzwPTstBeZFe/+rCMvRwr4yROQP43s0Xk"
      crossorigin="anonymous"
    ></script>
    <script>
      document.addEventListener("DOMContentLoaded", function () {
        renderMathInElement(document.body, {
          // customised options
          // • auto-render specific keys, e.g.:
          delimiters: [
            { left: "$$", right: "$$", display: true },
            { left: "$", right: "$", display: false },
            { left: "\\(", right: "\\)", display: false },
            { left: "\\[", right: "\\]", display: true },
          ],
          // • rendering keys, e.g.:
          throwOnError: false,
        });
      });
    </script>

    <!-- Google tag (gtag.js) -->
    <script
      async
      src="https://www.googletagmanager.com/gtag/js?id=G-E49FXX81PL"
    ></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() {
        dataLayer.push(arguments);
      }
      gtag("js", new Date());

      gtag("config", "G-E49FXX81PL");
    </script>
  </head>
  <body>
    <!-- Title. We tweak the wrapping behaviors a bit. -->
    <div style="height: 1em"></div>
    <h1 style="padding: 0 1em; white-space: nowrap">
      Estimating Body<wbr /> and Hand Motion<wbr /> in an <wbr />Ego-sensed
      World
    </h1>

    <!-- Authors. -->
    <div id="author-list">
      <a href=""> Brent Yi<sup>1</sup> </a>
      <a href="https://people.eecs.berkeley.edu/~vye/" target="_blank">
        Vickie Ye<sup>1</sup>
      </a>
      <a href="https://www.linkedin.com/in/maya-zheng/" target="_blank">
        Maya Zheng<sup>1</sup>
      </a>
      <a href="https://sites.google.com/view/muelea" target="_blank">
        Lea M&uuml;ller<sup>1</sup>
      </a>
      <a href="https://geopavlakos.github.io/" target="_blank">
        Georgios Pavlakos<sup>2</sup>
      </a>
      <a href="https://people.eecs.berkeley.edu/~yima/" target="_blank"
        >Yi Ma<sup>1</sup></a
      >
      <a href="https://people.eecs.berkeley.edu/~malik/" target="_blank">
        Jitendra Malik<sup>1</sup>
      </a>
      <a href="https://people.eecs.berkeley.edu/~kanazawa/" target="_blank">
        Angjoo Kanazawa<sup>1</sup>
      </a>
    </div>
    <div style="height: 0.5em"></div>

    <!-- Affiliations -->
    <div id="affiliations">
      <div><sup>1</sup> UC Berkeley</div>
      <div><sup>2</sup> UT Austin</div>
    </div>
    <div style="height: 1em"></div>

    <!-- Links -->
    <div
      style="
        display: flex;
        justify-content: center;
        gap: 0.5em 1em;
        flex-wrap: wrap;
      "
    >
      <a href="https://github.com/brentyi/egoallo" target="_blank">
        <button>
          <i class="ti ti-brand-github"></i>
          Code
        </button>
      </a>
      <a href="https://arxiv.org/abs/2410.03665" target="_blank">
        <button>
          <i class="ti ti-article"></i>
          arXiv
        </button>
      </a>
    </div>
    <div style="height: 0.5em"></div>

    <!-- tldr -->
    <section class="wide">
      <p style="text-align: center; max-width: 27em">
        <span class="highlight">
          <strong>TLDR;</strong>
          We use egocentric (
          <i class="ti ti-eyeglass" style="font-weight: 600"></i>
          ) SLAM poses and images to estimate 3D human body pose, height, and
          hands.
        </span>
      </p>

      <script>
        function toggleFullscreen(event) {
          event.preventDefault();
          const resultsSection = event.target
            .closest("section")
            .querySelector(".results");
          if (!document.fullscreenElement) {
            resultsSection
              .requestFullscreen()
              .then(() => {
                resultsSection.setAttribute("data-fullscreen", "true");
              })
              .catch((err) => {
                console.error(
                  `Error attempting to enable fullscreen: ${err.message}`,
                );
              });
          } else {
            document.exitFullscreen().then(() => {
              resultsSection.removeAttribute("data-fullscreen");
            });
          }
        }
      </script>

      <p style="text-align: center">
        Some results from our method, with input view on top-left:
      </p>
      <div class="results" style="position: relative">
        <div class="iframe-wrapper"></div>
        <button
          class="results-fullscreen-toggle"
          href="#"
          style="
            position: absolute;
            top: 0.5em;
            right: 0.5em;
            z-index: 1000;
            padding: 0.5em;
            line-height: 1em;
            border: 0.1em solid rgba(255, 255, 255, 0.3);
          "
          onclick="toggleFullscreen(event)"
        >
          <i class="ti ti-arrows-maximize"></i>
        </button>
        <div class="results-selector">
          <button class="results-prev">
            <i class="ti ti-chevron-left"></i>
          </button>
          <button class="results-next">
            <i class="ti ti-chevron-right"></i>
          </button>
          <div class="results-thumbnails"></div>
        </div>
      </div>
      <script src="results.js"></script>
      <p style="text-align: center">
        <i
          class="ti ti-hand-click"
          style="font-weight: 500; font-size: 1.3em"
        ></i>
        <span style="font-weight: 600"
          >The visualization above is interactive!</span
        >
      </p>
      <p style="text-align: center">
        Visualized scenes are outputs from
        <a href="https://nerf.studio">Nerfstudio</a>
        and
        <a
          href="https://facebookresearch.github.io/projectaria_tools/docs/ARK/mps"
        >
          Project Aria MPS</a
        >.
      </p>
    </section>

    <section>
      <h2>Overview</h2>
      <p>
        Our system, EgoAllo, uses <em>ego</em>centric observations to estimate
        the wearer of a head-mounted device's actions in the
        <em>allo</em>centric scene coordinate frame. To do this, we:
      </p>
      <ul>
        <li>
          Train a human motion and height prior conditioned on head motion.
        </li>
        <li>
          Guide sampling from the prior to align with visual hand observations.
        </li>
      </ul>
      <img src="./images/method.svg" style="width: 100%; margin: 1em 0" />
    </section>

    <section>
      <h2>Head Pose Conditioning</h2>
      <p>
        Our key insight for improving estimation is in conditioning
        representation.
      </p>
      <p>
        We aim to condition a human motion prior on head motion. Naively
        conditioning on absolute poses, however, would introduce sensitivity to
        arbitrary world frame choices. Consider these trajectories, which have
        identical local body motion but completely different absolute head
        poses:
      </p>
      <div style="margin: 2em 0; text-align: center">
        <img
          src="./images/conditioning/top0.png"
          style="width: 50%; vertical-align: top"
          class="full-width-on-mobile"
        />
        <img
          src="./images/conditioning/top2.png"
          style="width: 35%; vertical-align: top"
          class="full-width-on-mobile"
        />
      </div>
      <p>
        Training a model using these poses as conditioning would result in poor
        generalization, as inputs become susceptible to infinite possible world
        frame shifts.
      </p>
      <p>
        Prior works have solved this by aligning trajectories with their first
        frame. We observe, however, that canonicalizing sequences this way leads
        to sensitivity to
        <em>time</em>. Consider two slices of the same motion:
      </p>
      <div style="margin: 2em 0; text-align: center">
        <!-- I lower opacity here because when I rendered these I made the
          opacity of the humans too high... need to re-render. -->
        <img
          src="./images/conditioning/slice0.png"
          style="width: 40%; vertical-align: top; opacity: 0.8"
          class="full-width-on-mobile"
        />
        <img
          src="./images/conditioning/slice1.png"
          style="width: 40%; vertical-align: top; opacity: 0.8"
          class="full-width-on-mobile"
        />
      </div>
      <p>
        Head poses from canonicalized sequences can still differ significantly,
        even for the same body motion <span style="color: red">(circled)</span>:
      </p>
      <div style="margin: 2em 0; text-align: center">
        <img
          src="./images/conditioning/canonical_top0_circled.png"
          style="width: 36%; vertical-align: top"
          class="full-width-on-mobile"
        />
        <img
          src="./images/conditioning/canonical_top2_circled.png"
          style="width: 50%; vertical-align: top"
          class="full-width-on-mobile"
        />
      </div>
      <p>
        This also hinders generalization: networks must "re-learn" outputs for
        each slice of the input.
      </p>
      <p>
        Motivated by this, our paper proposes <strong>(1)</strong>&nbsp;spatial
        and temporal invariance properties that are desirable for head pose
        conditioning, and <strong>(2)</strong>&nbsp;an alternative
        parameterization that achieves them.
      </p>
      <p>
        Using the central pupil frame (CPF) to measure head motion, our
        invariant parameterization couples relative CPF motion
        $\Delta\mathbf{T}_\text{cpf}^t$ with <em>per-timestep</em> canonicalized
        pose $\mathbf{T}_{\text{canonical},\text{cpf}}^t$.
      </p>
      <img
        src="./images/conditioning/conditioning_annotated.svg"
        style="width: 100%; margin: 1em 0"
      />
      <p>
        These transformations have improved invariance properties over prior
        methods, while fully defining head pose trajectories relative to the
        floor plane.
      </p>
      <p>
        Quantitatively, this explains joint position estimation error
        differences between 5% and 18%.
      </p>
      <p>
        Qualitatively, we observe consistent improvements in realism. For
        example, see the subtle but critical improvements in foot motion for
        this dynamic sequence:
      </p>
      <video
        src="./results_videos/cond_side_by_side_smaller.mp4"
        style="display: block"
        width="100%"
        controls
        loop
        autoplay
        muted
      ></video>
      <span style="opacity: 0.5; margin: 0.75em 0; font-size: 0.8em">
        Trajectory source: EgoExo4D, unc_soccer_09-22-23_01_27.
      </span>
    </section>

    <section>
      <h2>Hand Guidance</h2>
      <p>
        Head motion encodes significant information about body motion, but
        articulated hands require a richer input. In EgoAllo, we extract visual
        hand observations using
        <a href="https://geopavlakos.github.io/hamer/">HaMeR</a> and
        (optionally) Project Aria's
        <a
          href="https://facebookresearch.github.io/projectaria_tools/docs/data_formats/mps/hand_tracking"
          >wrist and palm estimator</a
        >. We then incorporate these into sampling via diffusion guidance.
      </p>
      <p>
        We observe that jointly estimating human hands with bodies (<span
          style="color: purple"
          >purple</span
        >) reduces ambiguities and errors when compared to single-frame
        monocular estimates (<span style="color: teal">blue</span>):
      </p>
      <img
        src="./images/hands_vs_hamer.png"
        style="
          width: 100%;
          max-width: 30em;
          margin: 1.5em auto 2em auto;
          display: block;
        "
      />
      <p>
        Compared to naive HaMeR, EgoAllo with head pose + HaMeR input drops
        world-frame hand joint errors by as much as 40%.
      </p>
    </section>

    <section>
      <h2>Related links</h2>
      If this problem is interesting to you, here are some papers that you might
      enjoy!
      <ul>
        <li>
          <a href="https://siplab.org/projects/AvatarPoser">
            AvatarPoser: Articulated Full-Body Pose Tracking from Sparse Motion
            Sensing
          </a>
        </li>
        <li>
          <a href="https://lijiaman.github.io/projects/egoego/">
            Ego-Body Pose Estimation via Ego-Head Pose Estimation
          </a>
        </li>
        <li>
          <a href="https://arxiv.org/abs/2304.11118">
            BoDiffusion: Diffusing Sparse Observations for Full-Body Human
            Motion Synthesis
          </a>
        </li>
        <li>
          <a href="https://arxiv.org/abs/2308.06493">
            EgoPoser: Robust Real-Time Egocentric Pose Estimation from Sparse
            and Intermittent Observations Everywhere
          </a>
        </li>
        <li>
          <a href="https://arxiv.org/abs/2409.13426">
            HMD<sup>2</sup>: Environment-aware Motion Generation from Single
            Egocentric Head-Mounted Device
          </a>
        </li>
      </ul>
    </section>

    <section>
      <h2>Citation</h2>
      <code style="overflow-x: scroll; white-space: nowrap">
        @misc{yi2024estimatingbodyhandmotion,<br />
        &nbsp;&nbsp;&nbsp;&nbsp;title={Estimating Body and Hand Motion in an
        Ego-sensed World},<br />
        &nbsp;&nbsp;&nbsp;&nbsp;author={Brent Yi and Vickie Ye and Maya Zheng
        and Lea M\"uller and Georgios Pavlakos and Yi Ma and Jitendra Malik and
        Angjoo Kanazawa},<br />
        &nbsp;&nbsp;&nbsp;&nbsp;year={2024},<br />
        &nbsp;&nbsp;&nbsp;&nbsp;eprint={2410.03665},<br />
        &nbsp;&nbsp;&nbsp;&nbsp;archivePrefix={arXiv},<br />
        &nbsp;&nbsp;&nbsp;&nbsp;primaryClass={cs.CV},<br />
        &nbsp;&nbsp;&nbsp;&nbsp;url={https://arxiv.org/abs/2410.03665},<br />
        }
      </code>
    </section>
    <div style="height: 4em"></div>
  </body>
</html>
