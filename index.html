<!doctype html>
<html lang="en">
  <head>
    <title>EgoAllo</title>
    <meta
      content="We use egocentric SLAM poses and images to estimate 3D human body pose, height, and hands in the world."
      name="description"
    />

    <!-- Usual metadata. -->
    <meta charset="UTF-8" />
    <link href="./favicon.png" rel="shortcut icon" type="image/x-icon" />
    <meta
      name="viewport"
      content="width=device-width, initial-scale=1, minimum-scale=1"
    />

    <!-- Open Graph / Facebook. -->
    <meta property="og:title" content="EgoAllo" />
    <meta
      property="og:description"
      content="We use egocentric SLAM poses and images to estimate 3D human body pose, height, and hands in the world."
    />
    <meta property="og:type" content="website" />

    <!-- Twitter. -->
    <meta property="twitter:title" content="EgoAllo" />
    <meta
      property="twitter:description"
      content="We use egocentric SLAM poses and images to estimate 3D human body pose, height, and hands in the world."
    />

    <!-- Webfont. -->
    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
    <link
      href="https://fonts.googleapis.com/css2?family=Inter:ital,opsz,wght@0,14..32,100..900;1,14..32,100..900&display=swap"
      rel="stylesheet"
    />

    <!-- Styles. -->
    <link
      href="https://cdn.jsdelivr.net/npm/modern-normalize@3.0.1/modern-normalize.min.css"
      rel="stylesheet"
    />
    <link
      href="https://cdnjs.cloudflare.com/ajax/libs/tabler-icons/3.19.0/tabler-icons-outline.min.css"
      rel="stylesheet"
    />
    <link href="style.css" rel="stylesheet" type="text/css" />

    <!-- KaTeX -->
    <link
      rel="stylesheet"
      href="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.css"
      integrity="sha384-nB0miv6/jRmo5UMMR1wu3Gz6NLsoTkbqJghGIsx//Rlm+ZU03BU6SQNC66uf4l5+"
      crossorigin="anonymous"
    />
    <script
      defer
      src="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.js"
      integrity="sha384-7zkQWkzuo3B5mTepMUcHkMB5jZaolc2xDwL6VFqjFALcbeS9Ggm/Yr2r3Dy4lfFg"
      crossorigin="anonymous"
    ></script>
    <script
      defer
      src="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/contrib/auto-render.min.js"
      integrity="sha384-43gviWU0YVjaDtb/GhzOouOXtZMP/7XUzwPTstBeZFe/+rCMvRwr4yROQP43s0Xk"
      crossorigin="anonymous"
    ></script>
    <script>
      document.addEventListener("DOMContentLoaded", function () {
        renderMathInElement(document.body, {
          // customised options
          // • auto-render specific keys, e.g.:
          delimiters: [
            { left: "$$", right: "$$", display: true },
            { left: "$", right: "$", display: false },
            { left: "\\(", right: "\\)", display: false },
            { left: "\\[", right: "\\]", display: true },
          ],
          // • rendering keys, e.g.:
          throwOnError: false,
        });
      });
    </script>

    <!-- Google tag (gtag.js) -->
    <script
      async
      src="https://www.googletagmanager.com/gtag/js?id=G-E49FXX81PL"
    ></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() {
        dataLayer.push(arguments);
      }
      gtag("js", new Date());

      gtag("config", "G-E49FXX81PL");
    </script>
  </head>
  <body>
    <!-- Title. We tweak the wrapping behaviors a bit. -->
    <div style="height: 1em"></div>
    <h1 style="padding: 0 1em">
      <!-- &#8209; is a non-breaking hyphen. -->
      Estimating Body and Hand&nbsp;Motion in an Ego&#8209;sensed&nbsp;World
    </h1>

    <!-- Authors. -->
    <div id="author-list">
      <a href="https://brentyi.github.io"> Brent Yi<sup>1</sup> </a>
      <a href="https://people.eecs.berkeley.edu/~vye/" target="_blank">
        Vickie Ye<sup>1</sup>
      </a>
      <a href="https://www.linkedin.com/in/maya-zheng/" target="_blank">
        Maya Zheng<sup>1</sup>
      </a>
      <a href="https://annie-liyunqi.github.io" target="_blank">
        Yunqi Li<sup>2</sup>
      </a>
      <a href="https://sites.google.com/view/muelea" target="_blank">
        Lea M&uuml;ller<sup>1</sup>
      </a>
      <a href="https://geopavlakos.github.io/" target="_blank">
        Georgios Pavlakos<sup>3</sup>
      </a>
      <a href="https://people.eecs.berkeley.edu/~yima/" target="_blank"
        >Yi Ma<sup>1</sup></a
      >
      <a href="https://people.eecs.berkeley.edu/~malik/" target="_blank">
        Jitendra Malik<sup>1</sup>
      </a>
      <a href="https://people.eecs.berkeley.edu/~kanazawa/" target="_blank">
        Angjoo Kanazawa<sup>1</sup>
      </a>
    </div>
    <div style="height: 0.5em"></div>

    <!-- Affiliations -->
    <div id="affiliations">
      <div><sup>1</sup>&nbsp;UC Berkeley</div>
      <div><sup>2</sup>&nbsp;ShanghaiTech</div>
      <div><sup>3</sup>&nbsp;UT Austin</div>
    </div>
    <div style="height: 1em"></div>

    <!-- Links -->
    <div
      style="
        display: flex;
        justify-content: center;
        gap: 0.75em;
        flex-wrap: wrap;
      "
    >
      <a href="https://arxiv.org/abs/2410.03665" target="_blank">
        <button>
          <i class="ti ti-article"></i>
          arXiv
        </button>
      </a>
      <a href="./EgoAllo_December2024.pdf">
        <button>
          <i class="ti ti-file-type-pdf"></i>
          Paper
        </button>
      </a>
      <a href="https://github.com/brentyi/egoallo" target="_blank">
        <button>
          <i class="ti ti-brand-github"></i>
          Code
        </button>
      </a>
      <a href="./results.html" target="_blank">
        <button
          style="
            background: #8e2de2;
            /* background: linear-gradient(to right, #8e2de2, #4a00e0); */
            box-shadow: 0 0 0.75em 0 rgba(255, 255, 50, 1);
          "
        >
          <i class="ti ti-hand-click"></i>
          Interactive Results
        </button>
      </a>
    </div>
    <div style="height: 0.5em"></div>

    <!-- tldr -->
    <section class="wide">
      <p style="text-align: center; max-width: 30em">
        <strong>TLDR;</strong>
        We use egocentric (
        <i class="ti ti-eyeglass" style="font-weight: 600"></i>
        ) SLAM poses and images to estimate human body pose, height, and hands
        in the world.
      </p>
      <div style="height: 0.5em"></div>
      <video
        src="./egoallo_overview.mp4"
        width="100%"
        controls
        muted
        style="box-shadow: 0 0 1em rgba(0, 0, 0, 0.07)"
      ></video>
      <p style="text-align: center">
        Visualized scenes are outputs from
        <a href="https://nerf.studio">Nerfstudio</a>
        and
        <a
          href="https://facebookresearch.github.io/projectaria_tools/docs/ARK/mps"
        >
          Project Aria MPS</a
        >.
      </p>
    </section>
    <section>
      <h2>Interactive Results</h2>
      <p style="text-align: center">
        Click below to explore some example outputs!
      </p>
      <a
        href="./results.html"
        target="_blank"
        style="position: relative; display: block; overflow: hidden"
      >
        <video
          src="./interactive_demo.mp4"
          style="opacity: 50%; display: block"
          width="100%"
          loop
          autoplay
          playsinline
          muted
          id="interactive-demo-video"
        ></video>
        <button
          style="
            position: absolute;
            top: 50%;
            left: 50%;
            transform: translate(-50%, -50%);
            background: #8e2de2;
            /* background: linear-gradient(to right, #8e2de2, #4a00e0); */
            box-shadow: 0 0 2em 0 rgba(255, 255, 50, 1);
            whitespace: nowrap;
          "
        >
          <i class="ti ti-hand-click"></i>
          Open Interactive Results
        </button>
      </a>
      <script>
        // Make this video play slower.
        document.addEventListener("DOMContentLoaded", function () {
          var video = document.getElementById("interactive-demo-video");
          video.playbackRate = 0.6;
        });
      </script>
    </section>

    <section>
      <h2>Overview</h2>
      <p>
        Our system, EgoAllo, uses <em>ego</em>centric observations to estimate
        the wearer of a head-mounted device's actions in the
        <em>allo</em>centric scene coordinate frame. To do this, we:
      </p>
      <ul>
        <li>
          Train a human motion and height prior conditioned on head motion.
        </li>
        <li>
          Guide sampling from the prior to align with visual hand observations.
        </li>
      </ul>
      <img src="./images/method.svg" style="width: 100%; margin: 1em 0" />
    </section>

    <section>
      <h2>Invariant Conditioning for Learning</h2>
      <p>
        Our main insight for improving estimation is in conditioning
        representation.
      </p>
      <p>
        We aim to condition a human motion prior on head motion. Naively
        conditioning on absolute poses, however, would introduce sensitivity to
        arbitrary world frame choices. Consider these trajectories, which have
        identical local body motion but completely different absolute head
        poses:
      </p>
      <div style="margin: 2em 0; text-align: center">
        <img
          src="./images/conditioning/top0.png"
          style="width: 50%; vertical-align: top"
          class="full-width-on-mobile"
        />
        <img
          src="./images/conditioning/top2.png"
          style="width: 35%; vertical-align: top"
          class="full-width-on-mobile"
        />
      </div>
      <p>
        Training a model using these poses as conditioning would result in poor
        generalization, as inputs become susceptible to infinite possible world
        frame shifts.
      </p>
      <p>
        Prior works have solved this by aligning trajectories with their first
        frame. We observe, however, that canonicalizing sequences this way leads
        to sensitivity to
        <em>time</em>. Consider two slices of the same motion:
      </p>
      <div style="margin: 2em 0; text-align: center">
        <!-- I lower opacity here because when I rendered these I made the
          opacity of the humans too high... need to re-render. -->
        <img
          src="./images/conditioning/slice0.png"
          style="width: 40%; vertical-align: top; opacity: 0.8"
          class="full-width-on-mobile"
        />
        <img
          src="./images/conditioning/slice1.png"
          style="width: 40%; vertical-align: top; opacity: 0.8"
          class="full-width-on-mobile"
        />
      </div>
      <p>
        Head poses from canonicalized sequences can still differ significantly,
        even for the same body motion <span style="color: red">(circled)</span>:
      </p>
      <div style="margin: 2em 0; text-align: center">
        <img
          src="./images/conditioning/canonical_top0_circled.png"
          style="width: 36%; vertical-align: top"
          class="full-width-on-mobile"
        />
        <img
          src="./images/conditioning/canonical_top2_circled.png"
          style="width: 50%; vertical-align: top"
          class="full-width-on-mobile"
        />
      </div>
      <p>
        This also hinders generalization: networks must "re-learn" outputs for
        each slice of the input.
      </p>
      <p>
        Motivated by this, our paper proposes <strong>(1)</strong>&nbsp;spatial
        and temporal invariance properties that are desirable for head pose
        conditioning, and <strong>(2)</strong>&nbsp;an alternative
        parameterization that achieves them.
      </p>
      <p>
        Using the central pupil frame (CPF) to measure head motion, our
        invariant parameterization couples relative CPF motion
        $\Delta\mathbf{T}_\text{cpf}^t$ with <em>per-timestep</em> canonicalized
        pose $\mathbf{T}_{\text{canonical},\text{cpf}}^t$.
      </p>
      <img
        src="./images/conditioning/conditioning_annotated.svg"
        style="width: 100%; margin: 1em 0"
      />
      <p>
        These transformations have improved invariance properties over prior
        methods, while fully defining head pose trajectories relative to the
        floor plane.
      </p>
      <p>
        Quantitatively, this explains joint position estimation error
        differences between 5% and 18%.
      </p>
      <p>
        Qualitatively, we observe consistent improvements in realism. For
        example, see the subtle but critical improvements in foot motion for
        this dynamic sequence:
      </p>
      <video
        src="./results_videos/cond_side_by_side_smaller.mp4"
        style="display: block"
        width="100%"
        controls
        loop
        autoplay
        muted
      ></video>
      <span style="opacity: 0.5; margin: 0.75em 0; font-size: 0.8em">
        Trajectory source: EgoExo4D, unc_soccer_09-22-23_01_27.
      </span>
    </section>

    <section>
      <h2>Hand Guidance</h2>
      <p>
        Head motion encodes significant information about body motion, but
        articulated hands require a richer input. In EgoAllo, we extract visual
        hand observations using
        <a href="https://geopavlakos.github.io/hamer/">HaMeR</a> and
        (optionally) Project Aria's
        <a
          href="https://facebookresearch.github.io/projectaria_tools/docs/data_formats/mps/hand_tracking"
          >wrist and palm estimator</a
        >. We then incorporate these into sampling via diffusion guidance.
      </p>
      <p>
        We observe that jointly estimating human hands with bodies (<span
          style="color: purple"
          >purple</span
        >) reduces ambiguities and errors when compared to single-frame
        monocular estimates (<span style="color: teal">blue</span>):
      </p>
      <img
        src="./images/hands_vs_hamer.png"
        style="
          width: 100%;
          max-width: 30em;
          margin: 1.5em auto 2em auto;
          display: block;
        "
      />
      <p>
        Compared to naive HaMeR, EgoAllo with head pose + HaMeR input drops
        world-frame hand joint errors by as much as 40%.
      </p>
    </section>

    <section>
      <h2>Related links</h2>
      If this problem is interesting to you, here are some papers that you might
      enjoy!
      <ul>
        <li>
          <a href="https://siplab.org/projects/AvatarPoser">
            AvatarPoser: Articulated Full-Body Pose Tracking from Sparse Motion
            Sensing
          </a>
        </li>
        <li>
          <a href="https://lijiaman.github.io/projects/egoego/">
            Ego-Body Pose Estimation via Ego-Head Pose Estimation
          </a>
        </li>
        <li>
          <a href="https://arxiv.org/abs/2304.11118">
            BoDiffusion: Diffusing Sparse Observations for Full-Body Human
            Motion Synthesis
          </a>
        </li>
        <li>
          <a href="https://arxiv.org/abs/2308.06493">
            EgoPoser: Robust Real-Time Egocentric Pose Estimation from Sparse
            and Intermittent Observations Everywhere
          </a>
        </li>
        <li>
          <a href="https://arxiv.org/abs/2409.13426">
            HMD<sup>2</sup>: Environment-aware Motion Generation from Single
            Egocentric Head-Mounted Device
          </a>
        </li>
      </ul>
    </section>
    <section>
      <h2>Acknowledgements</h2>
      <p>
        We would like to thank Hongsuk Choi, Michael Taylor, Tyler Bonnen,
        Songwei Ge, Chung Min Kim, and Justin Kerr for insightful technical
        discussion and suggestions, as well as Jiaman Li for helpful answers to
        questions about EgoEgo.
      </p>
      <p>
        This project was funded in part by NSF:CNS-2235013 and IARPA DOI/IBC No.
        140D0423C0035. YM acknowledges support from the joint Simons
        Foundation-NSF DMS grant #2031899, the ONR grant N00014-22-1-2102, the
        NSF grant #2402951, and partial support from TBSI, InnoHK, and the
        University of Hong Kong. JM was supported by ONR MURI N00014-21-1-2801.
        BY is supported by the National Science Foundation Graduate Research
        Fellowship Program under Grant DGE 2146752.
      </p>
    </section>

    <section>
      <h2>Citation</h2>
      <code style="overflow-x: scroll; white-space: nowrap">
        @article{yi2024egoallo,<br />
        &nbsp;&nbsp;&nbsp;&nbsp;title={Estimating Body and Hand Motion in an
        Ego-sensed World},<br />
        &nbsp;&nbsp;&nbsp;&nbsp;author={Brent Yi and Vickie Ye and Maya Zheng
        and Lea M\"uller and Georgios Pavlakos and Yi Ma and Jitendra Malik and
        Angjoo Kanazawa},<br />
        &nbsp;&nbsp;&nbsp;&nbsp;year={2024},<br />
        &nbsp;&nbsp;&nbsp;&nbsp;journal={arXiv preprint arXiv:2410.03665},<br />
        }
      </code>
    </section>
    <div style="height: 4em"></div>
  </body>
</html>
