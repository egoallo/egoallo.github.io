<!doctype html>
<html lang="en">
  <head>
    <title>EgoAllo</title>

    <!-- Usual metadata. -->
    <meta charset="UTF-8" />
    <meta content="" name="description" />
    <meta content="Body " property="og:title" />
    <meta content="" property="og:description" />
    <meta content="" property="og:image" />
    <meta content="" property="twitter:title" />
    <meta content="" property="twitter:description" />
    <meta content="" property="twitter:image" />
    <meta property="og:type" content="website" />
    <meta content="summary_large_image" name="twitter:card" />
    <meta
      name="viewport"
      content="width=device-width, initial-scale=1, minimum-scale=1"
    />
    <link href="./favicon.png" rel="shortcut icon" type="image/x-icon" />
    <meta
      name="viewport"
      content="width=device-width, initial-scale=1, minimum-scale=1"
    />

    <!-- Webfont. -->
    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
    <link
      href="https://fonts.googleapis.com/css2?family=Inter:ital,opsz,wght@0,14..32,100..900;1,14..32,100..900&display=swap"
      rel="stylesheet"
    />

    <!-- Styles. -->
    <link
      href="https://cdn.jsdelivr.net/npm/modern-normalize@3.0.1/modern-normalize.min.css"
      rel="stylesheet"
    />
    <link
      rel="stylesheet"
      href="https://cdn.jsdelivr.net/npm/@tabler/icons@latest/iconfont/tabler-icons.min.css"
    />
    <link href="style.css" rel="stylesheet" type="text/css" />

    <!-- KaTeX -->
    <link
      rel="stylesheet"
      href="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.css"
      integrity="sha384-nB0miv6/jRmo5UMMR1wu3Gz6NLsoTkbqJghGIsx//Rlm+ZU03BU6SQNC66uf4l5+"
      crossorigin="anonymous"
    />
    <script
      defer
      src="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.js"
      integrity="sha384-7zkQWkzuo3B5mTepMUcHkMB5jZaolc2xDwL6VFqjFALcbeS9Ggm/Yr2r3Dy4lfFg"
      crossorigin="anonymous"
    ></script>
    <script
      defer
      src="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/contrib/auto-render.min.js"
      integrity="sha384-43gviWU0YVjaDtb/GhzOouOXtZMP/7XUzwPTstBeZFe/+rCMvRwr4yROQP43s0Xk"
      crossorigin="anonymous"
    ></script>
    <script>
      document.addEventListener("DOMContentLoaded", function () {
        renderMathInElement(document.body, {
          // customised options
          // • auto-render specific keys, e.g.:
          delimiters: [
            { left: "$$", right: "$$", display: true },
            { left: "$", right: "$", display: false },
            { left: "\\(", right: "\\)", display: false },
            { left: "\\[", right: "\\]", display: true },
          ],
          // • rendering keys, e.g.:
          throwOnError: false,
        });
      });
    </script>
  </head>
  <body>
    <!-- Title. We tweak the wrapping behaviors a bit. -->
    <div style="height: 1em"></div>
    <h1 style="padding: 0 1em; white-space: nowrap">
      Estimating Body<wbr /> and Hand Motion<wbr /> in an <wbr />Ego-sensed
      World
    </h1>

    <!-- Authors. -->
    <div id="author-list">
      <a target="_blank">Brent Yi<sup>1</sup></a>
      <a target="_blank">Vickie Ye<sup>1</sup></a>
      <a target="_blank">Maya Zheng<sup>1</sup></a>
      <a target="_blank">Lea M&uuml;ller<sup>1</sup></a>
      <a target="_blank">Georgios Pavlakos<sup>2</sup></a>
      <a target="_blank">Yi Ma<sup>1</sup></a>
      <a target="_blank">Jitendra Malik<sup>1</sup></a>
      <a target="_blank">Angjoo Kanazawa<sup>1</sup></a>
    </div>
    <div style="height: 0.5em"></div>

    <!-- Affiliations -->
    <div id="affiliations">
      <div><sup>1</sup> UC Berkeley</div>
      <div><sup>2</sup> UT Austin</div>
    </div>
    <div style="height: 1em"></div>

    <!-- Links -->
    <div
      style="
        display: flex;
        justify-content: center;
        gap: 0.5em 1em;
        flex-wrap: wrap;
      "
    >
      <a href="https://github.com/brentyi/egoallo" target="_blank">
        <button>
          <i class="ti ti-brand-github"></i>
          Code
        </button>
      </a>
      <a href="TODO" target="_blank">
        <button>
          <i class="ti ti-article"></i>
          arXiv
        </button>
      </a>
    </div>
    <div style="height: 0.5em"></div>

    <!-- tldr -->
    <section class="wide">
      <p style="text-align: center; max-width: 27em">
        <span class="highlight">
          <strong>TLDR;</strong>
          We use egocentric (
          <i class="ti ti-eyeglass" style="font-weight: 600"></i>
          ) SLAM poses and images to estimate 3D human body pose, height, and
          hands.
        </span>
      </p>

      <script>
        function toggleFullscreen(event) {
          event.preventDefault();
          const resultsSection = event.target
            .closest("section")
            .querySelector(".results");
          if (!document.fullscreenElement) {
            resultsSection
              .requestFullscreen()
              .then(() => {
                resultsSection.setAttribute("data-fullscreen", "true");
              })
              .catch((err) => {
                console.error(
                  `Error attempting to enable fullscreen: ${err.message}`,
                );
              });
          } else {
            document.exitFullscreen().then(() => {
              resultsSection.removeAttribute("data-fullscreen");
            });
          }
        }
      </script>

      <p style="text-align: center">
        Some results from our method, with input view on top-left:
      </p>
      <div class="results" style="position: relative">
        <div class="iframe-wrapper"></div>
        <button
          class="results-fullscreen-toggle"
          href="#"
          style="
            position: absolute;
            top: 0.5em;
            right: 0.5em;
            z-index: 1000;
            padding: 0.5em;
            line-height: 1em;
            border: 0.1em solid rgba(255, 255, 255, 0.3);
          "
          onclick="toggleFullscreen(event)"
        >
          <i class="ti ti-arrows-maximize"></i>
        </button>
        <div class="results-selector">
          <button class="results-prev">
            <i class="ti ti-chevron-left"></i>
          </button>
          <button class="results-next">
            <i class="ti ti-chevron-right"></i>
          </button>
          <div class="results-thumbnails"></div>
        </div>
      </div>
      <script src="results.js"></script>
      <p style="text-align: center">
        Scenes are visualized using outputs from
        <a href="https://nerf.studio">Nerfstudio</a>
        and
        <a
          href="https://facebookresearch.github.io/projectaria_tools/docs/ARK/mps"
        >
          Aria MPS </a
        >.
      </p>
    </section>

    <section>
      <h2>Overview</h2>
      <p>
        Our system, EgoAllo, uses <em>ego</em>centric observations to estimate
        the wearer's actions in the <em>allo</em>centric scene coordinate frame.
        To do this, we:
      </p>
      <ul>
        <li>
          Train a human motion and height prior conditioned on head motion.
        </li>
        <li>
          Guide sampling from the prior to align with visual hand observations.
        </li>
      </ul>
      <img src="./images/method.svg" style="width: 100%; margin: 1em 0" />
    </section>

    <section>
      <h2>Head Pose Conditioning</h2>
      <p>
        Our key insight for improving estimation is in head pose representation,
        which we use to condition a human motion diffusion model.
      </p>
      <p>
        Conditioning on absolute poses introduces sensitivity to world frame
        choice&mdash;consider these trajectories with identical local body
        motion but completely different absolute head poses:
      </p>
      <div style="margin: 2em 0; text-align: center">
        <img
          src="./images/conditioning/top0.png"
          style="width: 50%; vertical-align: top"
          class="full-width-on-mobile"
        />
        <img
          src="./images/conditioning/top2.png"
          style="width: 35%; vertical-align: top"
          class="full-width-on-mobile"
        />
      </div>
      <p>
        Prior works solve this by aligning trajectories based on their first
        timesteps. We observe, however, that canonicalizing sequences this way
        introduces a sensitivity to
        <em>time</em>. Consider two slices of the same motion:
      </p>
      <div style="margin: 2em 0; text-align: center">
        <!-- I lower opacity here because when I rendered these I made the
          opacity of the humans too high... need to re-render. -->
        <img
          src="./images/conditioning/slice0.png"
          style="width: 40%; vertical-align: top; opacity: 0.8"
          class="full-width-on-mobile"
        />
        <img
          src="./images/conditioning/slice1.png"
          style="width: 40%; vertical-align: top; opacity: 0.8"
          class="full-width-on-mobile"
        />
      </div>
      <p>
        Head poses from canonicalized sequences can still differ significantly,
        even for the same body motion <span style="color: red">(circled)</span>:
      </p>
      <div style="margin: 2em 0; text-align: center">
        <img
          src="./images/conditioning/canonical_top0_circled.png"
          style="width: 36%; vertical-align: top"
          class="full-width-on-mobile"
        />
        <img
          src="./images/conditioning/canonical_top2_circled.png"
          style="width: 50%; vertical-align: top"
          class="full-width-on-mobile"
        />
      </div>
      <p>
        To improve our model's ability to learn from data, we propose
        <strong>(1)</strong>&nbsp;spatial and temporal invariance properties for
        head pose conditioning, and <strong>(2)</strong>&nbsp;an alternative
        representation that achieves them.
      </p>
      <p>
        Our invariant parameterization couples relative central pupil frame
        (CPF) motion $\Delta\mathbf{T}_\text{cpf}^t$ with transformations
        between a <em>per-timestep</em> canonicalized pose
        $\mathbf{T}_{\text{canonical},\text{cpf}}^t$.
      </p>
      <img
        src="./images/conditioning/conditioning_annotated.svg"
        style="width: 100%; margin: 1em 0"
      />
      <p>
        In empirical evaluation, we find that this explains accuracy differences
        between 5% and 18%.
      </p>
    </section>

    <section>
      <h2>Related links</h2>
      If this problem is interesting to you, here are some papers from others
      that you might enjoy!
      <ul>
        <li>
          <a href="https://siplab.org/projects/AvatarPoser">
            AvatarPoser: Articulated Full-Body Pose Tracking from Sparse Motion
            Sensing
          </a>
        </li>
        <li>
          <a href="https://lijiaman.github.io/projects/egoego/">
            Ego-Body Pose Estimation via Ego-Head Pose Estimation
          </a>
        </li>
        <li>
          <a href="https://arxiv.org/abs/2304.11118">
            BoDiffusion: Diffusing Sparse Observations for Full-Body Human
            Motion Synthesis
          </a>
        </li>
        <li>
          <a href="https://arxiv.org/abs/2308.06493">
            EgoPoser: Robust Real-Time Egocentric Pose Estimation from Sparse
            and Intermittent Observations Everywhere
          </a>
        </li>
        <li>
          <a href="https://arxiv.org/abs/2409.13426">
            HMD<sup>2</sup>: Environment-aware Motion Generation from Single
            Egocentric Head-Mounted Device
          </a>
        </li>
      </ul>
    </section>

    <!-- Citation (TODO) -->
    <!--
    <section
      style="background-color: #f7f7f7; border-radius: 1em; padding: 1em 2em"
    >
      <h2>Citation</h2>
      <code style="overflow-x: scroll; white-space: nowrap">
        @inproceedings{yi2024egoallo,<br />
        &nbsp;&nbsp;&nbsp;&nbsp;author = {Yi, Brent and Ye, Vickie and Zheng,
        Maya and M\"uller, Lea and Pavlakos, Georgios and Ma, Yi and Malik,
        Jitendra and Kanazawa, Angjoo},<br />
        &nbsp;&nbsp;&nbsp;&nbsp;title = {Estimating Body and Hand Motion in an
        Ego-sensed World},<br />
        &nbsp;&nbsp;&nbsp;&nbsp;booktitle = {International Conference on
        Computer Vision (ICCV)},<br />
        &nbsp;&nbsp;&nbsp;&nbsp;year = {2023},<br />
        }
      </code>
    </section>
    -->
    <div style="height: 4em"></div>
  </body>
</html>
